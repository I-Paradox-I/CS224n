Prompting, Reinforcement Learning from Human Feedback *(by Jesse Mu)*
[[slides](https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf)]

Suggested Readings:

1. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
2. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
3. [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
4. [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)



Question Answering
[[slides](https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture12-QA.pdf)]

Suggested readings:

1. [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/pdf/1606.05250.pdf)
2. [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/pdf/1611.01603.pdf)
3. [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/pdf/1704.00051.pdf)
4. [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/pdf/1906.00300.pdf)
5. [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)
6. [Learning Dense Representations of Phrases at Scale](https://arxiv.org/pdf/2012.12624.pdf)



Project Milestone **out**
[[Instructions](https://web.stanford.edu/class/cs224n/project/CS224N_Final_Project_Milestone_Instructions.pdf)]